{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "the estimate of Qa, we have\nQn1a 1\nn 1n1X\nt1rt\n1\nn 1rn11\nn 1nX\nt1rt\n1\nn 1rn1n\nn 11\nnnX\nt1rt\n1\nn 1rn1n\nn 1Qna\nQna nrn1Qna,with n1\nn 1A basic algorithm for reinforcement learning\nSo, the learning algorithm is as follows\nThe agent starts with an arbitrarily estimated action value, e.g,\nQa  0aA.\nAt each time step t, the agent selects an action a, observes the\nreward r.\nUpdates Qa as follows\nQa Qa arQa 21Q-learning Extending the Basic Learning Algorithm to\nMDPs\nSingle-Decision Sequential-Decision\na s,a\nQa Qs,a\nQa ERa Qs,a E\nRs,a max\naAQs,a\nr rmax\naAQs,a\na s,a\nThe analogy of the learning algorithm\nQa Qa arQa 22\nis\nQs,a Qs,a s,a\nrmax\naAQs,aQs,a\n23Q-learning Extending the Basic Algorithm to MDPs\nThe basic learning algorithm can be extended to MDPs as follows\nInitialize action values Qs,a  0s,a.\nAt each time step t, observe state sand take action a.\nObserve the new state sand the gained reward r.\nUpdate action values as follows\nQs,aQs,a s,a\nrmax\naAQs,aQs,a\nThis defines the Q-learning algorithm for MDPs.Convergence of Q-learning\nTheorem\nIn a finite MDP, the Q-value function computed by Q-learning algorithm\nconverges to the optimal Q-function Qw.p.1 if the following conditions\nare satisfied\n1The Q-values are stored in a lookup table,\n2Every state-action pair is visited an infinite number of times,\n3The learning rate satisfies the following conditions\n1tst,at0,1,\n2P\nttst,at ,\n3P\nttst,at2w.p.1,\n4s,a st,at, ts,a  0 ,\n4Var\nRs,a,s\n.", "metadata": {"source": "./documents\\RL course by elmehdi amhraoui (1).txt"}}}