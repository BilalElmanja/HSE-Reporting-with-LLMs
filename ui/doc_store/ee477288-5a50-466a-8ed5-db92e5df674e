{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "St,Att0,R\nQuestion What is the reward function in the case of our particle?Markov decision process Putting all concepts together\nDefinition\nA Markov decision process MDP is a four tuple  S,A,T,R, where\nSis the set of all possible states of the environment,\nAis the set of all possible actions of the agent,\nTSAS0,1 is the transition model. That is\nPSt1sSts,Ata Ts,a,s\nRSASRis the reward function.\nQuestion Model the particle motion problem as an MDP.Solving an MDP\nSolving an MDP fundamentally equivalent to solving the following\nproblem\nmax\nEhH1X\nt0tRSt,At,St1i\nSubject to AtSt,t 1,2, . . . , H\nSt1TSt,At, .,t 1,2, . . . , H1\nS0T0.11\nHis referred to as the horizon.\nis known as the discount factor.\nIfHis finite, we can set  1 otherwise,  1.Action value function\nLetSA\nLetsSandaA.\nWe define the value of the action ain state sunder the policy ,\ndenoted as Qs,a, as the expected sum of rewards received by\nthe agent when starting from s, agent performs action aand\nfollows policy thereafter . That is\nQs,a ET,hH1X\nt0tRSt,At,St1S0s,A0ai\n12\nQSARis known as Q-function under the policy .\nWe define the optimal Q-function, denoted Q, as the maximum\nexpected sum of rewards an agent can gain when starting from sand\ntake the action a, that is\nQs,a  max\nQs,a 13Fundamental theorem of reinforcement learning\nTheorem\nLetM S,A,T,Rbe an MDP. If SandAare finite and Ris\nbounded, then\nThere is at least one deterministic policy that solves the MDP M.", "metadata": {"source": "./documents\\RL course by elmehdi amhraoui (1).txt"}}}