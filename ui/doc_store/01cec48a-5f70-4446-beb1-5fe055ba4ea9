{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "Bellman operator B, we have to proceed as follows\nfk1Bfkfk1Bfk 25\nk1 arg min\nE\nf Bfk2\n26\nk1arg min\n1\n D X\neDfs,a Bfks,a227\nk1arg min\n1\n D X\neD\nfs,armax\naAfks,a2\n z \nLk28\nTherefore, k1arg min\nLkQuestion Assuming that we use fSRA. What is the formula of the\nloss function Lk at iteration k.Policy gradient methods\nDQN does not directly estimate the policy instead, it approximates\nthe optimal Q-function by a parameterized Q-function.\nIn some cases, it would be better to estimate the optimal policy\ndirectly by utilizing a parameterized policy\nSA 29\nMethods that employ a parameterized policy are referred to as policy\ngradient methods.General Form of Policy Gradient Methods\nLetSA be a parameterized policy.\nLetJ be a performance measure that assesses the performance of\n.\nObjective Find such that J  max J.\nThe stochastic gradient descent SGD method in this case is given\nby\nJ 30General Form of Policy Gradient Methods\nQuestion Assume that the initial state is always s0. Suggest performance\nmeasure J such that arg max\nLis a good approximation of .General Form of Policy Gradient Methods\nWithout loss of generality, we can assume that we are in an episodic\ncase and the the initial state is always s0\nIn this case, we can take\nJ vs0 EH1X\nt0RSt,At,St1\f\fS0s0\n31\nTheorem\nLetbe a differentiable policy parametrization, we have\nJ E\nQs,alogas\n32REINFORCE\nAt time step t,J E\nQs,alogas\ncan be\nestimated as\nJ QSt,AtlogAtSt 33\nLetrt,rt1,,rHa sequence of rewards observed when starting", "metadata": {"source": "./documents\\RL course by elmehdi amhraoui (1).txt"}}}