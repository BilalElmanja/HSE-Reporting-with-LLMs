{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "1The Q-values are stored in a lookup table,\n2Every state-action pair is visited an infinite number of times,\n3The learning rate satisfies the following conditions\n1tst,at0,1,\n2P\nttst,at ,\n3P\nttst,at2w.p.1,\n4s,a st,at, ts,a  0 ,\n4Var\nRs,a,s\n.\nQuestion Find the optimal policy of the particle motion problem using\nQ-learning.\nConsider a scenario where our particle is constrained to move within a\nlimited space defined as space  0, . . . , 20   0, . . . , 20.Limitations of Q-learning\nLimitations\nQ-learning uses a table to store Q-function.\nMemory complexity cardScardA\nHigh complexity if state and action spaces are huge.\nLack of generalization Every state-action pair must be visited\ninfinitely, often impractical.\nSolution\nApproximate Qusing a function approximation, e.g., an Artificial\nNeural Network fSARorfSRA.\nChallenge Construct an algorithm to find such that fis a good\napproximation of Q.Limitations of Q-learning\nQuestion Given the following fact\nQs,a E\nRs,a max\naAQs,a\n24\nSuggest a loss function L such that farg min\nLis a good approximation\nofQ.Deep Q-network algorithm DQN\nD a set that contains the experiences of agent of the form\nes,a,s,r.\nk the model parameters at iteration k,\nfkfk.\nIn order for fkto converge to Q, which is the fixed point of the optimal\nBellman operator B, we have to proceed as follows\nfk1Bfkfk1Bfk 25\nk1 arg min\nE\nf Bfk2\n26\nk1arg min\n1\n D X\neDfs,a Bfks,a227\nk1arg min\n1\n D X\neD\nfs,armax\naAfks,a2\n z \nLk28\nTherefore, k1arg min", "metadata": {"source": "./documents\\RL course by elmehdi amhraoui (1).txt"}}}