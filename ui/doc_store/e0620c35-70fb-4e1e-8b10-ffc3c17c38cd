{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"page_content": "J vs0 EH1X\nt0RSt,At,St1\f\fS0s0\n31\nTheorem\nLetbe a differentiable policy parametrization, we have\nJ E\nQs,alogas\n32REINFORCE\nAt time step t,J E\nQs,alogas\ncan be\nestimated as\nJ QSt,AtlogAtSt 33\nLetrt,rt1,,rHa sequence of rewards observed when starting\nform St, taking the action Atand following thereafter. An\nestimate of QSt,At is\nGtrtrt1rH 34\nA new estimate of J is\nJ GtlogAtSt 35\nThe REINFORCE algorithm uses the following update rule of \nGtlogAtSt 36Limitation of REINFORCE and Actor-Critic Methods\n...", "metadata": {"source": "./documents\\RL course by elmehdi amhraoui (1).txt"}}}